{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "forest.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sak1b0/proteiNN/blob/master/forest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uhMU-BH1HBT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4918
        },
        "outputId": "4265e903-6418-4f02-d789-767aca1fc65b"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras import losses\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import argmax\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "prop = {1:[1.8,-0.17,0.11,0,0.38,-0.21,-1.6,0.42,-0.27,1.12,0.61],\n",
        "18:[-4.5,-0.81,2.58,3.71,-2.57,2.11,12.3,-1.56,1.87,-2.55,0.6],\n",
        "14:[-3.5,-0.42,2.05,3.47,-1.62,0.96,4.8,-1.03,0.81,-0.83,0.06],\n",
        "4:[-3.5,-1.23,3.49,2.95,-3.27,1.36,9.2,-0.51,0.81,-0.83,0.46],\n",
        "3:[2.5,0.24,-0.13,0.49,-0.3,-6.04,-2,0.84,-1.05,0.59,1.07],\n",
        "17:[-3.5,-0.58,2.36,3.01,-1.84,1.52,4.1,-0.96,1.1,-0.78,0],\n",
        "5:[-3.5,-2.02,2.68,1.64,-2.9,2.3,8.2,-0.37,1.17,-0.92,0.47],\n",
        "7:[-0.4,-0.01,0.74,1.72,-0.19,0,-1,0,-0.16,1.2,0.07],\n",
        "8:[-3.2,-0.96,2.06,4.76,-1.44,-1.23,3,-2.28,0.28,-0.93,0.61],\n",
        "9:[4.5,0.31,-0.6,-1.56,1.97,-4.81,-3.1,1.81,-0.77,1.16,2.22],\n",
        "12:[3.8,0.56,-0.55,-1.81,1.82,-4.68,-2.8,1.8,-1.1,1.18,1.53],\n",
        "11:[-3.9,-0.99,2.71,5.39,-3.46,3.88,8.8,-2.03,1.7,-0.8,1.15],\n",
        "13:[1.9,0.23,-0.1,-0.76,1.4,-3.66,-3.4,1.18,-0.73,0.55,1.18],\n",
        "6:[2.8,1.13,-0.32,-2.2,1.98,-4.65,-3.7,1.74,-1.43,0.67,2.02],\n",
        "16:[-1.6,-0.45,2.23,-1.52,-1.44,0.75,0.2,0.86,-0.75,0.54,1.95],\n",
        "19:[-0.8,-0.13,0.84,1.83,-0.53,1.74,-0.6,-0.64,0.42,-0.05,0.05],\n",
        "20:[-0.7,-0.14,0.52,1.78,-0.32,0.78,-1.2,-0.26,0.63,-0.02,0.05],\n",
        "23:[-0.9,1.85,0.3,-0.38,1.53,-3.32,-1.9,1.46,-1.57,-0.19,2.65],\n",
        "25:[-1.3,0.94,0.68,-1.09,0.49,-1.01,0.7,0.51,-0.56,-0.23,1.88],\n",
        "22:[4.2,-0.07,-0.31,-0.78,1.46,-3.5,-2.6,1.34,-0.4,1.13,1.32],\n",
        "26:[0,0,0,0,0,0,0,0,0,0,0]}\n",
        "\n",
        "df_train=np.asarray(pd.read_csv('https://raw.githubusercontent.com/sak1b0/proteiNN/master/train_formatted.csv',header=None))\n",
        "df_test=np.asarray(pd.read_csv('https://raw.githubusercontent.com/sak1b0/proteiNN/master/test_formatted.csv',header=None))\n",
        "\n",
        "x_train = df_train[:,0]\n",
        "y_train = df_train[:,1]\n",
        "\n",
        "x_test = df_test[:,0]\n",
        "y_test = df_test[:,1]\n",
        "\n",
        "def debug_me():\n",
        "  #print('train dataframe: ',df_train.shape)\n",
        "  print('x_train shape: ',x_train.shape)\n",
        "  print('y_train shape: ',y_train.shape)\n",
        "\n",
        "  #print('test dataframe: ',df_test.shape)\n",
        "  print('x_test shape: ',x_test.shape)\n",
        "  print('y_test shape: ',y_test.shape)\n",
        "\n",
        "max_len=400\n",
        "\n",
        "#================== x_train ===============\n",
        "n = x_train\n",
        "j=-1\n",
        "\n",
        "for i in x_train:\n",
        "  j=j+1\n",
        "  if(len(i)>max_len):\n",
        "    n = np.delete(n, j)\n",
        "    j=j-1\n",
        "\n",
        "for item in range (len(n)):\n",
        "  n[item] = n[item]+'Z'*(max_len-len(n[item]))\n",
        "\n",
        "x_train = n\n",
        "\n",
        "#================= y_train =================\n",
        "n = y_train\n",
        "j=-1\n",
        "\n",
        "for i in y_train:\n",
        "  j=j+1\n",
        "  if(len(i)>max_len):\n",
        "    n = np.delete(n, j)\n",
        "    j=j-1\n",
        "\n",
        "for item in range (len(n)):\n",
        "  n[item] = n[item]+'Z'*(max_len-len(n[item]))\n",
        "    \n",
        "y_train = n\n",
        "#=================  x_test ==================\n",
        "n = x_test\n",
        "j=-1\n",
        "\n",
        "for i in x_test:\n",
        "  j=j+1\n",
        "  if(len(i)>max_len):\n",
        "    n = np.delete(n, j)\n",
        "    j=j-1\n",
        "\n",
        "for item in range (len(n)):\n",
        "  n[item] = n[item]+'Z'*(max_len-len(n[item]))\n",
        "\n",
        "x_test = n\n",
        "#=================  y_test ==================\n",
        "n = y_test\n",
        "j=-1\n",
        "\n",
        "for i in y_test:\n",
        "  j=j+1\n",
        "  if(len(i)>max_len):\n",
        "    n = np.delete(n, j)\n",
        "    j=j-1\n",
        "\n",
        "for item in range (len(n)):\n",
        "  n[item] = n[item]+'Z'*(max_len-len(n[item]))\n",
        "\n",
        "y_test = n\n",
        "\n",
        "#============= selected data withing range===========\n",
        "\n",
        "\n",
        "max_len = max([len(i) for i in x_train])\n",
        "#print(max_len)\n",
        "\n",
        "max_len = max([len(i) for i in y_test])\n",
        "#print(max_len)\n",
        "\n",
        "print('starting the preprocessing\\n')\n",
        "start_time = time.time()\n",
        "\n",
        "#==============   Properties Encoded start  ============================\n",
        "\n",
        "# ==========x_train conversion start====\n",
        "s = list(x_train)\n",
        "\n",
        "k = []\n",
        "\n",
        "for i in range(len(s)):\n",
        "  t=[]\n",
        "  for item in range(len(s[i])):\n",
        "    t.append(prop[ord(s[i][item])-64])\n",
        "  k.append(t)\n",
        "\n",
        "\n",
        "x_train = np.array(k)\n",
        "\n",
        "#=========== x_train conversion end ====\n",
        "\n",
        "#=========== x_test conversion start====\n",
        "s = list(x_test)\n",
        "\n",
        "k = []\n",
        "\n",
        "for i in range(len(s)):\n",
        "  t=[]\n",
        "  for item in range(len(s[i])):\n",
        "    t.append(prop[ord(s[i][item])-64])\n",
        "  k.append(t)\n",
        "\n",
        "\n",
        "x_test = np.array(k)\n",
        "\n",
        "#============= x_test conversion end====\n",
        "\n",
        "\n",
        "#==============   Properties Encoded end  ============================\n",
        "\n",
        "\n",
        "\n",
        "#==============   ONE_HOT   ===================================================\n",
        "\n",
        "#======= y_train start========\n",
        "#y_train = y_train[0:3]\n",
        "\n",
        "alphabet = 'CEHXZ'\n",
        "\n",
        "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
        "\n",
        "k = []\n",
        "\n",
        "for i in range(len(y_train)):\n",
        "  integer_encoded = [char_to_int[char] for char in y_train[i]]\n",
        "  \n",
        "  onehot_encoded=list()\n",
        "  for value in integer_encoded:\n",
        "\t  letter = [0 for _ in range(len(alphabet))]\n",
        "\t  letter[value] = 1\n",
        "\t  onehot_encoded.append(letter)\n",
        "  \n",
        "  k.append(onehot_encoded)  \n",
        "\n",
        "y_train = np.array(k)\n",
        "#display(y_train)\n",
        "\n",
        "#======= y_train end========\n",
        "\n",
        "#======= y_test start========\n",
        "#y_train = y_train[0:3]\n",
        "\n",
        "alphabet = 'CEHXZ'\n",
        "\n",
        "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
        "\n",
        "k = []\n",
        "\n",
        "for i in range(len(y_test)):\n",
        "  integer_encoded = [char_to_int[char] for char in y_test[i]]\n",
        "  \n",
        "  onehot_encoded=list()\n",
        "  for value in integer_encoded:\n",
        "\t  letter = [0 for _ in range(len(alphabet))]\n",
        "\t  letter[value] = 1\n",
        "\t  onehot_encoded.append(letter)\n",
        "  \n",
        "  k.append(onehot_encoded)  \n",
        "\n",
        "y_test = np.array(k)\n",
        "#display(y_train)\n",
        "\n",
        "#======= y_test end========\n",
        "\n",
        "#==============   ONE_HOT   finish ============================\n",
        "\n",
        "print('ending the preprocessing\\n')\n",
        "finish_time=time.time()\n",
        "print ('Time taken to pre-process: ',round(finish_time - start_time,2),' seconds')\n",
        "\n",
        "#==============   ONE_HOT_INVERSION   =========================================\n",
        " \n",
        "#for i in range(len(y_train[0])):\n",
        "#  inverted = int_to_char[argmax(y_train[0][i])]\n",
        "#  print(inverted)\n",
        "\n",
        "#================ it's time to learn============================\n",
        "debug_me()\n",
        "start_time = time.time()\n",
        "  \n",
        "model=Sequential()\n",
        "\n",
        "model.add(LSTM((5),batch_input_shape=(None,400,11),return_sequences=True,activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "print(model.input_shape)\n",
        "print(model.output_shape)\n",
        "\n",
        "history=model.fit(x_train,y_train,epochs=500,validation_data=(x_test,y_test))\n",
        "\n",
        "finish_time=time.time()\n",
        "print ('Time taken to train: ',round((finish_time - start_time)/60,2),' minutes')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting the preprocessing\n",
            "\n",
            "ending the preprocessing\n",
            "\n",
            "Time taken to pre-process:  7.15  seconds\n",
            "x_train shape:  (4061, 400, 11)\n",
            "y_train shape:  (4061, 400, 5)\n",
            "x_test shape:  (1058, 400, 11)\n",
            "y_test shape:  (1058, 400, 5)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_13 (LSTM)               (None, 400, 5)            340       \n",
            "=================================================================\n",
            "Total params: 340\n",
            "Trainable params: 340\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "(None, 400, 11)\n",
            "(None, 400, 5)\n",
            "Train on 4061 samples, validate on 1058 samples\n",
            "Epoch 1/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 2.3136 - acc: 0.6537 - val_loss: 1.9413 - val_acc: 0.7151\n",
            "Epoch 2/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 1.6911 - acc: 0.7074 - val_loss: 1.4924 - val_acc: 0.7290\n",
            "Epoch 3/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 1.3318 - acc: 0.7153 - val_loss: 1.0061 - val_acc: 0.7371\n",
            "Epoch 4/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.9911 - acc: 0.7212 - val_loss: 0.9112 - val_acc: 0.7395\n",
            "Epoch 5/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.9578 - acc: 0.7229 - val_loss: 0.8971 - val_acc: 0.7406\n",
            "Epoch 6/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.9474 - acc: 0.7243 - val_loss: 0.8892 - val_acc: 0.7416\n",
            "Epoch 7/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.9404 - acc: 0.7259 - val_loss: 0.8840 - val_acc: 0.7421\n",
            "Epoch 8/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.9351 - acc: 0.7271 - val_loss: 0.8793 - val_acc: 0.7438\n",
            "Epoch 9/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.9305 - acc: 0.7282 - val_loss: 0.8753 - val_acc: 0.7448\n",
            "Epoch 10/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.9264 - acc: 0.7297 - val_loss: 0.8719 - val_acc: 0.7459\n",
            "Epoch 11/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.9231 - acc: 0.7306 - val_loss: 0.8694 - val_acc: 0.7468\n",
            "Epoch 12/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.9199 - acc: 0.7318 - val_loss: 0.8667 - val_acc: 0.7476\n",
            "Epoch 13/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.9174 - acc: 0.7325 - val_loss: 0.8642 - val_acc: 0.7488\n",
            "Epoch 14/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.9149 - acc: 0.7332 - val_loss: 0.8622 - val_acc: 0.7492\n",
            "Epoch 15/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.9130 - acc: 0.7337 - val_loss: 0.8606 - val_acc: 0.7498\n",
            "Epoch 16/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.9113 - acc: 0.7345 - val_loss: 0.8593 - val_acc: 0.7503\n",
            "Epoch 17/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.9100 - acc: 0.7348 - val_loss: 0.8579 - val_acc: 0.7510\n",
            "Epoch 18/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.9086 - acc: 0.7356 - val_loss: 0.8571 - val_acc: 0.7508\n",
            "Epoch 19/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.9073 - acc: 0.7361 - val_loss: 0.8559 - val_acc: 0.7516\n",
            "Epoch 20/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.9061 - acc: 0.7365 - val_loss: 0.8544 - val_acc: 0.7523\n",
            "Epoch 21/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.9049 - acc: 0.7371 - val_loss: 0.8534 - val_acc: 0.7529\n",
            "Epoch 22/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.9038 - acc: 0.7376 - val_loss: 0.8523 - val_acc: 0.7532\n",
            "Epoch 23/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.9027 - acc: 0.7380 - val_loss: 0.8514 - val_acc: 0.7537\n",
            "Epoch 24/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.9016 - acc: 0.7387 - val_loss: 0.8502 - val_acc: 0.7541\n",
            "Epoch 25/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.9004 - acc: 0.7392 - val_loss: 0.8492 - val_acc: 0.7545\n",
            "Epoch 26/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.8492 - acc: 0.7383 - val_loss: 0.7794 - val_acc: 0.7551\n",
            "Epoch 27/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.8194 - acc: 0.7405 - val_loss: 0.7752 - val_acc: 0.7564\n",
            "Epoch 28/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.8127 - acc: 0.7429 - val_loss: 0.7698 - val_acc: 0.7571\n",
            "Epoch 29/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.8096 - acc: 0.7431 - val_loss: 0.7676 - val_acc: 0.7582\n",
            "Epoch 30/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.8084 - acc: 0.7435 - val_loss: 0.7668 - val_acc: 0.7582\n",
            "Epoch 31/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.8078 - acc: 0.7437 - val_loss: 0.7667 - val_acc: 0.7575\n",
            "Epoch 32/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.8071 - acc: 0.7437 - val_loss: 0.7660 - val_acc: 0.7585\n",
            "Epoch 33/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.8065 - acc: 0.7442 - val_loss: 0.7652 - val_acc: 0.7589\n",
            "Epoch 34/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.8056 - acc: 0.7449 - val_loss: 0.7647 - val_acc: 0.7594\n",
            "Epoch 35/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.8046 - acc: 0.7456 - val_loss: 0.7641 - val_acc: 0.7594\n",
            "Epoch 36/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.8037 - acc: 0.7458 - val_loss: 0.7636 - val_acc: 0.7598\n",
            "Epoch 37/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.8032 - acc: 0.7458 - val_loss: 0.7633 - val_acc: 0.7592\n",
            "Epoch 38/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.8028 - acc: 0.7459 - val_loss: 0.7629 - val_acc: 0.7595\n",
            "Epoch 39/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.7431 - acc: 0.7457 - val_loss: 0.5253 - val_acc: 0.7591\n",
            "Epoch 40/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5537 - acc: 0.7456 - val_loss: 0.5225 - val_acc: 0.7592\n",
            "Epoch 41/500\n",
            "4061/4061 [==============================] - 82s 20ms/step - loss: 0.5517 - acc: 0.7460 - val_loss: 0.5216 - val_acc: 0.7598\n",
            "Epoch 42/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5507 - acc: 0.7462 - val_loss: 0.5204 - val_acc: 0.7600\n",
            "Epoch 43/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5499 - acc: 0.7466 - val_loss: 0.5199 - val_acc: 0.7598\n",
            "Epoch 44/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5494 - acc: 0.7466 - val_loss: 0.5195 - val_acc: 0.7605\n",
            "Epoch 45/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5488 - acc: 0.7468 - val_loss: 0.5187 - val_acc: 0.7607\n",
            "Epoch 46/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5482 - acc: 0.7471 - val_loss: 0.5189 - val_acc: 0.7601\n",
            "Epoch 47/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5479 - acc: 0.7471 - val_loss: 0.5181 - val_acc: 0.7606\n",
            "Epoch 48/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5473 - acc: 0.7475 - val_loss: 0.5187 - val_acc: 0.7592\n",
            "Epoch 49/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5472 - acc: 0.7474 - val_loss: 0.5177 - val_acc: 0.7604\n",
            "Epoch 50/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5465 - acc: 0.7477 - val_loss: 0.5168 - val_acc: 0.7610\n",
            "Epoch 51/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5463 - acc: 0.7476 - val_loss: 0.5167 - val_acc: 0.7609\n",
            "Epoch 52/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5459 - acc: 0.7477 - val_loss: 0.5163 - val_acc: 0.7616\n",
            "Epoch 53/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5456 - acc: 0.7478 - val_loss: 0.5161 - val_acc: 0.7616\n",
            "Epoch 54/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5453 - acc: 0.7478 - val_loss: 0.5155 - val_acc: 0.7616\n",
            "Epoch 55/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5449 - acc: 0.7481 - val_loss: 0.5154 - val_acc: 0.7618\n",
            "Epoch 56/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5447 - acc: 0.7479 - val_loss: 0.5154 - val_acc: 0.7620\n",
            "Epoch 57/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5443 - acc: 0.7483 - val_loss: 0.5151 - val_acc: 0.7618\n",
            "Epoch 58/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5441 - acc: 0.7484 - val_loss: 0.5150 - val_acc: 0.7614\n",
            "Epoch 59/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5439 - acc: 0.7483 - val_loss: 0.5145 - val_acc: 0.7615\n",
            "Epoch 60/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5438 - acc: 0.7484 - val_loss: 0.5144 - val_acc: 0.7620\n",
            "Epoch 61/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5435 - acc: 0.7485 - val_loss: 0.5142 - val_acc: 0.7623\n",
            "Epoch 62/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5432 - acc: 0.7486 - val_loss: 0.5142 - val_acc: 0.7615\n",
            "Epoch 63/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5429 - acc: 0.7487 - val_loss: 0.5138 - val_acc: 0.7623\n",
            "Epoch 64/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5428 - acc: 0.7488 - val_loss: 0.5136 - val_acc: 0.7625\n",
            "Epoch 65/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5426 - acc: 0.7489 - val_loss: 0.5135 - val_acc: 0.7626\n",
            "Epoch 66/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5423 - acc: 0.7490 - val_loss: 0.5134 - val_acc: 0.7628\n",
            "Epoch 67/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5420 - acc: 0.7492 - val_loss: 0.5139 - val_acc: 0.7620\n",
            "Epoch 68/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5421 - acc: 0.7491 - val_loss: 0.5130 - val_acc: 0.7629\n",
            "Epoch 69/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5419 - acc: 0.7493 - val_loss: 0.5134 - val_acc: 0.7618\n",
            "Epoch 70/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5417 - acc: 0.7494 - val_loss: 0.5127 - val_acc: 0.7628\n",
            "Epoch 71/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5413 - acc: 0.7499 - val_loss: 0.5123 - val_acc: 0.7629\n",
            "Epoch 72/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5412 - acc: 0.7497 - val_loss: 0.5122 - val_acc: 0.7629\n",
            "Epoch 73/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5409 - acc: 0.7500 - val_loss: 0.5120 - val_acc: 0.7632\n",
            "Epoch 74/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5407 - acc: 0.7499 - val_loss: 0.5117 - val_acc: 0.7632\n",
            "Epoch 75/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5407 - acc: 0.7499 - val_loss: 0.5122 - val_acc: 0.7632\n",
            "Epoch 76/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5403 - acc: 0.7504 - val_loss: 0.5117 - val_acc: 0.7634\n",
            "Epoch 77/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5401 - acc: 0.7502 - val_loss: 0.5118 - val_acc: 0.7637\n",
            "Epoch 78/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5400 - acc: 0.7505 - val_loss: 0.5115 - val_acc: 0.7634\n",
            "Epoch 79/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5397 - acc: 0.7505 - val_loss: 0.5112 - val_acc: 0.7636\n",
            "Epoch 80/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5395 - acc: 0.7506 - val_loss: 0.5108 - val_acc: 0.7639\n",
            "Epoch 81/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5392 - acc: 0.7508 - val_loss: 0.5107 - val_acc: 0.7640\n",
            "Epoch 82/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5389 - acc: 0.7510 - val_loss: 0.5104 - val_acc: 0.7641\n",
            "Epoch 83/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5387 - acc: 0.7511 - val_loss: 0.5101 - val_acc: 0.7643\n",
            "Epoch 84/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5386 - acc: 0.7511 - val_loss: 0.5104 - val_acc: 0.7642\n",
            "Epoch 85/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5384 - acc: 0.7513 - val_loss: 0.5096 - val_acc: 0.7648\n",
            "Epoch 86/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5380 - acc: 0.7515 - val_loss: 0.5093 - val_acc: 0.7650\n",
            "Epoch 87/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5375 - acc: 0.7521 - val_loss: 0.5089 - val_acc: 0.7653\n",
            "Epoch 88/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5371 - acc: 0.7521 - val_loss: 0.5090 - val_acc: 0.7651\n",
            "Epoch 89/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5368 - acc: 0.7523 - val_loss: 0.5087 - val_acc: 0.7649\n",
            "Epoch 90/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5368 - acc: 0.7524 - val_loss: 0.5089 - val_acc: 0.7649\n",
            "Epoch 91/500\n",
            "4061/4061 [==============================] - 82s 20ms/step - loss: 0.5362 - acc: 0.7527 - val_loss: 0.5082 - val_acc: 0.7655\n",
            "Epoch 92/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5361 - acc: 0.7528 - val_loss: 0.5080 - val_acc: 0.7661\n",
            "Epoch 93/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5357 - acc: 0.7531 - val_loss: 0.5078 - val_acc: 0.7655\n",
            "Epoch 94/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5356 - acc: 0.7532 - val_loss: 0.5074 - val_acc: 0.7664\n",
            "Epoch 95/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5353 - acc: 0.7533 - val_loss: 0.5083 - val_acc: 0.7658\n",
            "Epoch 96/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5350 - acc: 0.7537 - val_loss: 0.5073 - val_acc: 0.7661\n",
            "Epoch 97/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5347 - acc: 0.7538 - val_loss: 0.5064 - val_acc: 0.7665\n",
            "Epoch 98/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5345 - acc: 0.7540 - val_loss: 0.5070 - val_acc: 0.7659\n",
            "Epoch 99/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5343 - acc: 0.7542 - val_loss: 0.5079 - val_acc: 0.7650\n",
            "Epoch 100/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5344 - acc: 0.7542 - val_loss: 0.5075 - val_acc: 0.7660\n",
            "Epoch 101/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5338 - acc: 0.7547 - val_loss: 0.5058 - val_acc: 0.7672\n",
            "Epoch 102/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5336 - acc: 0.7549 - val_loss: 0.5064 - val_acc: 0.7663\n",
            "Epoch 103/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5336 - acc: 0.7547 - val_loss: 0.5055 - val_acc: 0.7672\n",
            "Epoch 104/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5332 - acc: 0.7549 - val_loss: 0.5053 - val_acc: 0.7675\n",
            "Epoch 105/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5331 - acc: 0.7550 - val_loss: 0.5053 - val_acc: 0.7673\n",
            "Epoch 106/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5328 - acc: 0.7554 - val_loss: 0.5058 - val_acc: 0.7674\n",
            "Epoch 107/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5326 - acc: 0.7552 - val_loss: 0.5053 - val_acc: 0.7675\n",
            "Epoch 108/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5324 - acc: 0.7556 - val_loss: 0.5046 - val_acc: 0.7680\n",
            "Epoch 109/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5325 - acc: 0.7554 - val_loss: 0.5046 - val_acc: 0.7678\n",
            "Epoch 110/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5320 - acc: 0.7559 - val_loss: 0.5044 - val_acc: 0.7679\n",
            "Epoch 111/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5318 - acc: 0.7559 - val_loss: 0.5041 - val_acc: 0.7678\n",
            "Epoch 112/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5319 - acc: 0.7557 - val_loss: 0.5041 - val_acc: 0.7682\n",
            "Epoch 113/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5317 - acc: 0.7561 - val_loss: 0.5175 - val_acc: 0.7609\n",
            "Epoch 114/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5333 - acc: 0.7551 - val_loss: 0.5039 - val_acc: 0.7682\n",
            "Epoch 115/500\n",
            "4061/4061 [==============================] - 82s 20ms/step - loss: 0.5315 - acc: 0.7561 - val_loss: 0.5035 - val_acc: 0.7684\n",
            "Epoch 116/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5313 - acc: 0.7561 - val_loss: 0.5037 - val_acc: 0.7681\n",
            "Epoch 117/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5308 - acc: 0.7564 - val_loss: 0.5032 - val_acc: 0.7685\n",
            "Epoch 118/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5306 - acc: 0.7567 - val_loss: 0.5032 - val_acc: 0.7683\n",
            "Epoch 119/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5309 - acc: 0.7566 - val_loss: 0.5035 - val_acc: 0.7685\n",
            "Epoch 120/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5306 - acc: 0.7567 - val_loss: 0.5031 - val_acc: 0.7689\n",
            "Epoch 121/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5308 - acc: 0.7565 - val_loss: 0.5031 - val_acc: 0.7683\n",
            "Epoch 122/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5302 - acc: 0.7568 - val_loss: 0.5029 - val_acc: 0.7687\n",
            "Epoch 123/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5306 - acc: 0.7564 - val_loss: 0.5027 - val_acc: 0.7686\n",
            "Epoch 124/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5300 - acc: 0.7570 - val_loss: 0.5028 - val_acc: 0.7690\n",
            "Epoch 125/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5299 - acc: 0.7572 - val_loss: 0.5022 - val_acc: 0.7692\n",
            "Epoch 126/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5302 - acc: 0.7570 - val_loss: 0.5023 - val_acc: 0.7692\n",
            "Epoch 127/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5297 - acc: 0.7573 - val_loss: 0.5021 - val_acc: 0.7691\n",
            "Epoch 128/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5296 - acc: 0.7575 - val_loss: 0.5031 - val_acc: 0.7688\n",
            "Epoch 129/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5296 - acc: 0.7574 - val_loss: 0.5021 - val_acc: 0.7695\n",
            "Epoch 130/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5307 - acc: 0.7568 - val_loss: 0.5020 - val_acc: 0.7692\n",
            "Epoch 131/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5292 - acc: 0.7578 - val_loss: 0.5017 - val_acc: 0.7698\n",
            "Epoch 132/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5291 - acc: 0.7577 - val_loss: 0.5014 - val_acc: 0.7698\n",
            "Epoch 133/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5292 - acc: 0.7576 - val_loss: 0.5015 - val_acc: 0.7698\n",
            "Epoch 134/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5291 - acc: 0.7581 - val_loss: 0.5018 - val_acc: 0.7700\n",
            "Epoch 135/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5289 - acc: 0.7582 - val_loss: 0.5013 - val_acc: 0.7698\n",
            "Epoch 136/500\n",
            "2144/4061 [==============>...............] - ETA: 34s - loss: 0.5358 - acc: 0.7552"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pE5tuzfRS9SK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXOucdnVW4mF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random as rnd\n",
        "row=rnd.randint(1,500)\n",
        "clm=rnd.randint(1,100)\n",
        "\n",
        "display(y_test[row][clm])\n",
        "display(y_pred[row][clm])\n",
        "plt.plot(y_test[row][clm])\n",
        "plt.plot(y_pred[row][clm])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}