{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "forest.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sak1b0/proteiNN/blob/master/forest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uhMU-BH1HBT",
        "colab_type": "code",
        "outputId": "29e3a549-e7b9-4920-80d3-e867e5d66207",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11460
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras import losses\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import argmax\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "prop = {1:[1.8,-0.17,0.11,0,0.38,-0.21,-1.6,0.42,-0.27,1.12,0.61],\n",
        "18:[-4.5,-0.81,2.58,3.71,-2.57,2.11,12.3,-1.56,1.87,-2.55,0.6],\n",
        "14:[-3.5,-0.42,2.05,3.47,-1.62,0.96,4.8,-1.03,0.81,-0.83,0.06],\n",
        "4:[-3.5,-1.23,3.49,2.95,-3.27,1.36,9.2,-0.51,0.81,-0.83,0.46],\n",
        "3:[2.5,0.24,-0.13,0.49,-0.3,-6.04,-2,0.84,-1.05,0.59,1.07],\n",
        "17:[-3.5,-0.58,2.36,3.01,-1.84,1.52,4.1,-0.96,1.1,-0.78,0],\n",
        "5:[-3.5,-2.02,2.68,1.64,-2.9,2.3,8.2,-0.37,1.17,-0.92,0.47],\n",
        "7:[-0.4,-0.01,0.74,1.72,-0.19,0,-1,0,-0.16,1.2,0.07],\n",
        "8:[-3.2,-0.96,2.06,4.76,-1.44,-1.23,3,-2.28,0.28,-0.93,0.61],\n",
        "9:[4.5,0.31,-0.6,-1.56,1.97,-4.81,-3.1,1.81,-0.77,1.16,2.22],\n",
        "12:[3.8,0.56,-0.55,-1.81,1.82,-4.68,-2.8,1.8,-1.1,1.18,1.53],\n",
        "11:[-3.9,-0.99,2.71,5.39,-3.46,3.88,8.8,-2.03,1.7,-0.8,1.15],\n",
        "13:[1.9,0.23,-0.1,-0.76,1.4,-3.66,-3.4,1.18,-0.73,0.55,1.18],\n",
        "6:[2.8,1.13,-0.32,-2.2,1.98,-4.65,-3.7,1.74,-1.43,0.67,2.02],\n",
        "16:[-1.6,-0.45,2.23,-1.52,-1.44,0.75,0.2,0.86,-0.75,0.54,1.95],\n",
        "19:[-0.8,-0.13,0.84,1.83,-0.53,1.74,-0.6,-0.64,0.42,-0.05,0.05],\n",
        "20:[-0.7,-0.14,0.52,1.78,-0.32,0.78,-1.2,-0.26,0.63,-0.02,0.05],\n",
        "23:[-0.9,1.85,0.3,-0.38,1.53,-3.32,-1.9,1.46,-1.57,-0.19,2.65],\n",
        "25:[-1.3,0.94,0.68,-1.09,0.49,-1.01,0.7,0.51,-0.56,-0.23,1.88],\n",
        "22:[4.2,-0.07,-0.31,-0.78,1.46,-3.5,-2.6,1.34,-0.4,1.13,1.32],\n",
        "26:[0,0,0,0,0,0,0,0,0,0,0]}\n",
        "\n",
        "df_train=np.asarray(pd.read_csv('https://raw.githubusercontent.com/sak1b0/proteiNN/master/train_formatted.csv',header=None))\n",
        "df_test=np.asarray(pd.read_csv('https://raw.githubusercontent.com/sak1b0/proteiNN/master/test_formatted.csv',header=None))\n",
        "\n",
        "x_train = df_train[:,0]\n",
        "y_train = df_train[:,1]\n",
        "\n",
        "x_test = df_test[:,0]\n",
        "y_test = df_test[:,1]\n",
        "\n",
        "def debug_me():\n",
        "  #print('train dataframe: ',df_train.shape)\n",
        "  print('x_train shape: ',x_train.shape)\n",
        "  print('y_train shape: ',y_train.shape)\n",
        "\n",
        "  #print('test dataframe: ',df_test.shape)\n",
        "  print('x_test shape: ',x_test.shape)\n",
        "  print('y_test shape: ',y_test.shape)\n",
        "\n",
        "max_len=400\n",
        "\n",
        "#================== x_train ===============\n",
        "n = x_train\n",
        "j=-1\n",
        "\n",
        "for i in x_train:\n",
        "  j=j+1\n",
        "  if(len(i)>max_len):\n",
        "    n = np.delete(n, j)\n",
        "    j=j-1\n",
        "\n",
        "for item in range (len(n)):\n",
        "  n[item] = n[item]+'Z'*(max_len-len(n[item]))\n",
        "\n",
        "x_train = n\n",
        "\n",
        "#================= y_train =================\n",
        "n = y_train\n",
        "j=-1\n",
        "\n",
        "for i in y_train:\n",
        "  j=j+1\n",
        "  if(len(i)>max_len):\n",
        "    n = np.delete(n, j)\n",
        "    j=j-1\n",
        "\n",
        "for item in range (len(n)):\n",
        "  n[item] = n[item]+'Z'*(max_len-len(n[item]))\n",
        "    \n",
        "y_train = n\n",
        "#=================  x_test ==================\n",
        "n = x_test\n",
        "j=-1\n",
        "\n",
        "for i in x_test:\n",
        "  j=j+1\n",
        "  if(len(i)>max_len):\n",
        "    n = np.delete(n, j)\n",
        "    j=j-1\n",
        "\n",
        "for item in range (len(n)):\n",
        "  n[item] = n[item]+'Z'*(max_len-len(n[item]))\n",
        "\n",
        "x_test = n\n",
        "#=================  y_test ==================\n",
        "n = y_test\n",
        "j=-1\n",
        "\n",
        "for i in y_test:\n",
        "  j=j+1\n",
        "  if(len(i)>max_len):\n",
        "    n = np.delete(n, j)\n",
        "    j=j-1\n",
        "\n",
        "for item in range (len(n)):\n",
        "  n[item] = n[item]+'Z'*(max_len-len(n[item]))\n",
        "\n",
        "y_test = n\n",
        "\n",
        "#============= selected data withing range===========\n",
        "\n",
        "\n",
        "max_len = max([len(i) for i in x_train])\n",
        "#print(max_len)\n",
        "\n",
        "max_len = max([len(i) for i in y_test])\n",
        "#print(max_len)\n",
        "\n",
        "print('starting the preprocessing\\n')\n",
        "start_time = time.time()\n",
        "\n",
        "#==============   Properties Encoded start  ============================\n",
        "\n",
        "# ==========x_train conversion start====\n",
        "s = list(x_train)\n",
        "\n",
        "k = []\n",
        "\n",
        "for i in range(len(s)):\n",
        "  t=[]\n",
        "  for item in range(len(s[i])):\n",
        "    t.append(prop[ord(s[i][item])-64])\n",
        "  k.append(t)\n",
        "\n",
        "\n",
        "x_train = np.array(k)\n",
        "\n",
        "#=========== x_train conversion end ====\n",
        "\n",
        "#=========== x_test conversion start====\n",
        "s = list(x_test)\n",
        "\n",
        "k = []\n",
        "\n",
        "for i in range(len(s)):\n",
        "  t=[]\n",
        "  for item in range(len(s[i])):\n",
        "    t.append(prop[ord(s[i][item])-64])\n",
        "  k.append(t)\n",
        "\n",
        "\n",
        "x_test = np.array(k)\n",
        "\n",
        "#============= x_test conversion end====\n",
        "\n",
        "\n",
        "#==============   Properties Encoded end  ============================\n",
        "\n",
        "\n",
        "\n",
        "#==============   ONE_HOT   ===================================================\n",
        "\n",
        "#======= y_train start========\n",
        "#y_train = y_train[0:3]\n",
        "\n",
        "alphabet = 'CEHXZ'\n",
        "\n",
        "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
        "\n",
        "k = []\n",
        "\n",
        "for i in range(len(y_train)):\n",
        "  integer_encoded = [char_to_int[char] for char in y_train[i]]\n",
        "  \n",
        "  onehot_encoded=list()\n",
        "  for value in integer_encoded:\n",
        "\t  letter = [0 for _ in range(len(alphabet))]\n",
        "\t  letter[value] = 1\n",
        "\t  onehot_encoded.append(letter)\n",
        "  \n",
        "  k.append(onehot_encoded)  \n",
        "\n",
        "y_train = np.array(k)\n",
        "#display(y_train)\n",
        "\n",
        "#======= y_train end========\n",
        "\n",
        "#======= y_test start========\n",
        "#y_train = y_train[0:3]\n",
        "\n",
        "alphabet = 'CEHXZ'\n",
        "\n",
        "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
        "\n",
        "k = []\n",
        "\n",
        "for i in range(len(y_test)):\n",
        "  integer_encoded = [char_to_int[char] for char in y_test[i]]\n",
        "  \n",
        "  onehot_encoded=list()\n",
        "  for value in integer_encoded:\n",
        "\t  letter = [0 for _ in range(len(alphabet))]\n",
        "\t  letter[value] = 1\n",
        "\t  onehot_encoded.append(letter)\n",
        "  \n",
        "  k.append(onehot_encoded)  \n",
        "\n",
        "y_test = np.array(k)\n",
        "#display(y_train)\n",
        "\n",
        "#======= y_test end========\n",
        "\n",
        "#==============   ONE_HOT   finish ============================\n",
        "\n",
        "print('ending the preprocessing\\n')\n",
        "finish_time=time.time()\n",
        "print ('Time taken to pre-process: ',round(finish_time - start_time,2),' seconds')\n",
        "\n",
        "#==============   ONE_HOT_INVERSION   =========================================\n",
        " \n",
        "#for i in range(len(y_train[0])):\n",
        "#  inverted = int_to_char[argmax(y_train[0][i])]\n",
        "#  print(inverted)\n",
        "\n",
        "#================ it's time to learn============================\n",
        "debug_me()\n",
        "start_time = time.time()\n",
        "  \n",
        "model=Sequential()\n",
        "\n",
        "model.add(LSTM((5),batch_input_shape=(None,400,11),return_sequences=True,activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "print(model.input_shape)\n",
        "print(model.output_shape)\n",
        "\n",
        "history=model.fit(x_train,y_train,epochs=500,validation_data=(x_test,y_test))\n",
        "\n",
        "finish_time=time.time()\n",
        "print ('Time taken to train: ',round((finish_time - start_time)/60,2),' minutes')\n"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting the preprocessing\n",
            "\n",
            "ending the preprocessing\n",
            "\n",
            "Time taken to pre-process:  7.15  seconds\n",
            "x_train shape:  (4061, 400, 11)\n",
            "y_train shape:  (4061, 400, 5)\n",
            "x_test shape:  (1058, 400, 11)\n",
            "y_test shape:  (1058, 400, 5)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_13 (LSTM)               (None, 400, 5)            340       \n",
            "=================================================================\n",
            "Total params: 340\n",
            "Trainable params: 340\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "(None, 400, 11)\n",
            "(None, 400, 5)\n",
            "Train on 4061 samples, validate on 1058 samples\n",
            "Epoch 1/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 2.3136 - acc: 0.6537 - val_loss: 1.9413 - val_acc: 0.7151\n",
            "Epoch 2/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 1.6911 - acc: 0.7074 - val_loss: 1.4924 - val_acc: 0.7290\n",
            "Epoch 3/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 1.3318 - acc: 0.7153 - val_loss: 1.0061 - val_acc: 0.7371\n",
            "Epoch 4/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.9911 - acc: 0.7212 - val_loss: 0.9112 - val_acc: 0.7395\n",
            "Epoch 5/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.9578 - acc: 0.7229 - val_loss: 0.8971 - val_acc: 0.7406\n",
            "Epoch 6/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.9474 - acc: 0.7243 - val_loss: 0.8892 - val_acc: 0.7416\n",
            "Epoch 7/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.9404 - acc: 0.7259 - val_loss: 0.8840 - val_acc: 0.7421\n",
            "Epoch 8/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.9351 - acc: 0.7271 - val_loss: 0.8793 - val_acc: 0.7438\n",
            "Epoch 9/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.9305 - acc: 0.7282 - val_loss: 0.8753 - val_acc: 0.7448\n",
            "Epoch 10/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.9264 - acc: 0.7297 - val_loss: 0.8719 - val_acc: 0.7459\n",
            "Epoch 11/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.9231 - acc: 0.7306 - val_loss: 0.8694 - val_acc: 0.7468\n",
            "Epoch 12/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.9199 - acc: 0.7318 - val_loss: 0.8667 - val_acc: 0.7476\n",
            "Epoch 13/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.9174 - acc: 0.7325 - val_loss: 0.8642 - val_acc: 0.7488\n",
            "Epoch 14/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.9149 - acc: 0.7332 - val_loss: 0.8622 - val_acc: 0.7492\n",
            "Epoch 15/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.9130 - acc: 0.7337 - val_loss: 0.8606 - val_acc: 0.7498\n",
            "Epoch 16/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.9113 - acc: 0.7345 - val_loss: 0.8593 - val_acc: 0.7503\n",
            "Epoch 17/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.9100 - acc: 0.7348 - val_loss: 0.8579 - val_acc: 0.7510\n",
            "Epoch 18/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.9086 - acc: 0.7356 - val_loss: 0.8571 - val_acc: 0.7508\n",
            "Epoch 19/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.9073 - acc: 0.7361 - val_loss: 0.8559 - val_acc: 0.7516\n",
            "Epoch 20/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.9061 - acc: 0.7365 - val_loss: 0.8544 - val_acc: 0.7523\n",
            "Epoch 21/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.9049 - acc: 0.7371 - val_loss: 0.8534 - val_acc: 0.7529\n",
            "Epoch 22/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.9038 - acc: 0.7376 - val_loss: 0.8523 - val_acc: 0.7532\n",
            "Epoch 23/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.9027 - acc: 0.7380 - val_loss: 0.8514 - val_acc: 0.7537\n",
            "Epoch 24/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.9016 - acc: 0.7387 - val_loss: 0.8502 - val_acc: 0.7541\n",
            "Epoch 25/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.9004 - acc: 0.7392 - val_loss: 0.8492 - val_acc: 0.7545\n",
            "Epoch 26/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.8492 - acc: 0.7383 - val_loss: 0.7794 - val_acc: 0.7551\n",
            "Epoch 27/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.8194 - acc: 0.7405 - val_loss: 0.7752 - val_acc: 0.7564\n",
            "Epoch 28/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.8127 - acc: 0.7429 - val_loss: 0.7698 - val_acc: 0.7571\n",
            "Epoch 29/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.8096 - acc: 0.7431 - val_loss: 0.7676 - val_acc: 0.7582\n",
            "Epoch 30/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.8084 - acc: 0.7435 - val_loss: 0.7668 - val_acc: 0.7582\n",
            "Epoch 31/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.8078 - acc: 0.7437 - val_loss: 0.7667 - val_acc: 0.7575\n",
            "Epoch 32/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.8071 - acc: 0.7437 - val_loss: 0.7660 - val_acc: 0.7585\n",
            "Epoch 33/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.8065 - acc: 0.7442 - val_loss: 0.7652 - val_acc: 0.7589\n",
            "Epoch 34/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.8056 - acc: 0.7449 - val_loss: 0.7647 - val_acc: 0.7594\n",
            "Epoch 35/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.8046 - acc: 0.7456 - val_loss: 0.7641 - val_acc: 0.7594\n",
            "Epoch 36/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.8037 - acc: 0.7458 - val_loss: 0.7636 - val_acc: 0.7598\n",
            "Epoch 37/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.8032 - acc: 0.7458 - val_loss: 0.7633 - val_acc: 0.7592\n",
            "Epoch 38/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.8028 - acc: 0.7459 - val_loss: 0.7629 - val_acc: 0.7595\n",
            "Epoch 39/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.7431 - acc: 0.7457 - val_loss: 0.5253 - val_acc: 0.7591\n",
            "Epoch 40/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5537 - acc: 0.7456 - val_loss: 0.5225 - val_acc: 0.7592\n",
            "Epoch 41/500\n",
            "4061/4061 [==============================] - 82s 20ms/step - loss: 0.5517 - acc: 0.7460 - val_loss: 0.5216 - val_acc: 0.7598\n",
            "Epoch 42/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5507 - acc: 0.7462 - val_loss: 0.5204 - val_acc: 0.7600\n",
            "Epoch 43/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5499 - acc: 0.7466 - val_loss: 0.5199 - val_acc: 0.7598\n",
            "Epoch 44/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5494 - acc: 0.7466 - val_loss: 0.5195 - val_acc: 0.7605\n",
            "Epoch 45/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5488 - acc: 0.7468 - val_loss: 0.5187 - val_acc: 0.7607\n",
            "Epoch 46/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5482 - acc: 0.7471 - val_loss: 0.5189 - val_acc: 0.7601\n",
            "Epoch 47/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5479 - acc: 0.7471 - val_loss: 0.5181 - val_acc: 0.7606\n",
            "Epoch 48/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5473 - acc: 0.7475 - val_loss: 0.5187 - val_acc: 0.7592\n",
            "Epoch 49/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5472 - acc: 0.7474 - val_loss: 0.5177 - val_acc: 0.7604\n",
            "Epoch 50/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5465 - acc: 0.7477 - val_loss: 0.5168 - val_acc: 0.7610\n",
            "Epoch 51/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5463 - acc: 0.7476 - val_loss: 0.5167 - val_acc: 0.7609\n",
            "Epoch 52/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5459 - acc: 0.7477 - val_loss: 0.5163 - val_acc: 0.7616\n",
            "Epoch 53/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5456 - acc: 0.7478 - val_loss: 0.5161 - val_acc: 0.7616\n",
            "Epoch 54/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5453 - acc: 0.7478 - val_loss: 0.5155 - val_acc: 0.7616\n",
            "Epoch 55/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5449 - acc: 0.7481 - val_loss: 0.5154 - val_acc: 0.7618\n",
            "Epoch 56/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5447 - acc: 0.7479 - val_loss: 0.5154 - val_acc: 0.7620\n",
            "Epoch 57/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5443 - acc: 0.7483 - val_loss: 0.5151 - val_acc: 0.7618\n",
            "Epoch 58/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5441 - acc: 0.7484 - val_loss: 0.5150 - val_acc: 0.7614\n",
            "Epoch 59/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5439 - acc: 0.7483 - val_loss: 0.5145 - val_acc: 0.7615\n",
            "Epoch 60/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5438 - acc: 0.7484 - val_loss: 0.5144 - val_acc: 0.7620\n",
            "Epoch 61/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5435 - acc: 0.7485 - val_loss: 0.5142 - val_acc: 0.7623\n",
            "Epoch 62/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5432 - acc: 0.7486 - val_loss: 0.5142 - val_acc: 0.7615\n",
            "Epoch 63/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5429 - acc: 0.7487 - val_loss: 0.5138 - val_acc: 0.7623\n",
            "Epoch 64/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5428 - acc: 0.7488 - val_loss: 0.5136 - val_acc: 0.7625\n",
            "Epoch 65/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5426 - acc: 0.7489 - val_loss: 0.5135 - val_acc: 0.7626\n",
            "Epoch 66/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5423 - acc: 0.7490 - val_loss: 0.5134 - val_acc: 0.7628\n",
            "Epoch 67/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5420 - acc: 0.7492 - val_loss: 0.5139 - val_acc: 0.7620\n",
            "Epoch 68/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5421 - acc: 0.7491 - val_loss: 0.5130 - val_acc: 0.7629\n",
            "Epoch 69/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5419 - acc: 0.7493 - val_loss: 0.5134 - val_acc: 0.7618\n",
            "Epoch 70/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5417 - acc: 0.7494 - val_loss: 0.5127 - val_acc: 0.7628\n",
            "Epoch 71/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5413 - acc: 0.7499 - val_loss: 0.5123 - val_acc: 0.7629\n",
            "Epoch 72/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5412 - acc: 0.7497 - val_loss: 0.5122 - val_acc: 0.7629\n",
            "Epoch 73/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5409 - acc: 0.7500 - val_loss: 0.5120 - val_acc: 0.7632\n",
            "Epoch 74/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5407 - acc: 0.7499 - val_loss: 0.5117 - val_acc: 0.7632\n",
            "Epoch 75/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5407 - acc: 0.7499 - val_loss: 0.5122 - val_acc: 0.7632\n",
            "Epoch 76/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5403 - acc: 0.7504 - val_loss: 0.5117 - val_acc: 0.7634\n",
            "Epoch 77/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5401 - acc: 0.7502 - val_loss: 0.5118 - val_acc: 0.7637\n",
            "Epoch 78/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5400 - acc: 0.7505 - val_loss: 0.5115 - val_acc: 0.7634\n",
            "Epoch 79/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5397 - acc: 0.7505 - val_loss: 0.5112 - val_acc: 0.7636\n",
            "Epoch 80/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5395 - acc: 0.7506 - val_loss: 0.5108 - val_acc: 0.7639\n",
            "Epoch 81/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5392 - acc: 0.7508 - val_loss: 0.5107 - val_acc: 0.7640\n",
            "Epoch 82/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5389 - acc: 0.7510 - val_loss: 0.5104 - val_acc: 0.7641\n",
            "Epoch 83/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5387 - acc: 0.7511 - val_loss: 0.5101 - val_acc: 0.7643\n",
            "Epoch 84/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5386 - acc: 0.7511 - val_loss: 0.5104 - val_acc: 0.7642\n",
            "Epoch 85/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5384 - acc: 0.7513 - val_loss: 0.5096 - val_acc: 0.7648\n",
            "Epoch 86/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5380 - acc: 0.7515 - val_loss: 0.5093 - val_acc: 0.7650\n",
            "Epoch 87/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5375 - acc: 0.7521 - val_loss: 0.5089 - val_acc: 0.7653\n",
            "Epoch 88/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5371 - acc: 0.7521 - val_loss: 0.5090 - val_acc: 0.7651\n",
            "Epoch 89/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5368 - acc: 0.7523 - val_loss: 0.5087 - val_acc: 0.7649\n",
            "Epoch 90/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5368 - acc: 0.7524 - val_loss: 0.5089 - val_acc: 0.7649\n",
            "Epoch 91/500\n",
            "4061/4061 [==============================] - 82s 20ms/step - loss: 0.5362 - acc: 0.7527 - val_loss: 0.5082 - val_acc: 0.7655\n",
            "Epoch 92/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5361 - acc: 0.7528 - val_loss: 0.5080 - val_acc: 0.7661\n",
            "Epoch 93/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5357 - acc: 0.7531 - val_loss: 0.5078 - val_acc: 0.7655\n",
            "Epoch 94/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5356 - acc: 0.7532 - val_loss: 0.5074 - val_acc: 0.7664\n",
            "Epoch 95/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5353 - acc: 0.7533 - val_loss: 0.5083 - val_acc: 0.7658\n",
            "Epoch 96/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5350 - acc: 0.7537 - val_loss: 0.5073 - val_acc: 0.7661\n",
            "Epoch 97/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5347 - acc: 0.7538 - val_loss: 0.5064 - val_acc: 0.7665\n",
            "Epoch 98/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5345 - acc: 0.7540 - val_loss: 0.5070 - val_acc: 0.7659\n",
            "Epoch 99/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5343 - acc: 0.7542 - val_loss: 0.5079 - val_acc: 0.7650\n",
            "Epoch 100/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5344 - acc: 0.7542 - val_loss: 0.5075 - val_acc: 0.7660\n",
            "Epoch 101/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5338 - acc: 0.7547 - val_loss: 0.5058 - val_acc: 0.7672\n",
            "Epoch 102/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5336 - acc: 0.7549 - val_loss: 0.5064 - val_acc: 0.7663\n",
            "Epoch 103/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5336 - acc: 0.7547 - val_loss: 0.5055 - val_acc: 0.7672\n",
            "Epoch 104/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5332 - acc: 0.7549 - val_loss: 0.5053 - val_acc: 0.7675\n",
            "Epoch 105/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5331 - acc: 0.7550 - val_loss: 0.5053 - val_acc: 0.7673\n",
            "Epoch 106/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5328 - acc: 0.7554 - val_loss: 0.5058 - val_acc: 0.7674\n",
            "Epoch 107/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5326 - acc: 0.7552 - val_loss: 0.5053 - val_acc: 0.7675\n",
            "Epoch 108/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5324 - acc: 0.7556 - val_loss: 0.5046 - val_acc: 0.7680\n",
            "Epoch 109/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5325 - acc: 0.7554 - val_loss: 0.5046 - val_acc: 0.7678\n",
            "Epoch 110/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5320 - acc: 0.7559 - val_loss: 0.5044 - val_acc: 0.7679\n",
            "Epoch 111/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5318 - acc: 0.7559 - val_loss: 0.5041 - val_acc: 0.7678\n",
            "Epoch 112/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5319 - acc: 0.7557 - val_loss: 0.5041 - val_acc: 0.7682\n",
            "Epoch 113/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5317 - acc: 0.7561 - val_loss: 0.5175 - val_acc: 0.7609\n",
            "Epoch 114/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5333 - acc: 0.7551 - val_loss: 0.5039 - val_acc: 0.7682\n",
            "Epoch 115/500\n",
            "4061/4061 [==============================] - 82s 20ms/step - loss: 0.5315 - acc: 0.7561 - val_loss: 0.5035 - val_acc: 0.7684\n",
            "Epoch 116/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5313 - acc: 0.7561 - val_loss: 0.5037 - val_acc: 0.7681\n",
            "Epoch 117/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5308 - acc: 0.7564 - val_loss: 0.5032 - val_acc: 0.7685\n",
            "Epoch 118/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5306 - acc: 0.7567 - val_loss: 0.5032 - val_acc: 0.7683\n",
            "Epoch 119/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5309 - acc: 0.7566 - val_loss: 0.5035 - val_acc: 0.7685\n",
            "Epoch 120/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5306 - acc: 0.7567 - val_loss: 0.5031 - val_acc: 0.7689\n",
            "Epoch 121/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5308 - acc: 0.7565 - val_loss: 0.5031 - val_acc: 0.7683\n",
            "Epoch 122/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5302 - acc: 0.7568 - val_loss: 0.5029 - val_acc: 0.7687\n",
            "Epoch 123/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5306 - acc: 0.7564 - val_loss: 0.5027 - val_acc: 0.7686\n",
            "Epoch 124/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5300 - acc: 0.7570 - val_loss: 0.5028 - val_acc: 0.7690\n",
            "Epoch 125/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5299 - acc: 0.7572 - val_loss: 0.5022 - val_acc: 0.7692\n",
            "Epoch 126/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5302 - acc: 0.7570 - val_loss: 0.5023 - val_acc: 0.7692\n",
            "Epoch 127/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5297 - acc: 0.7573 - val_loss: 0.5021 - val_acc: 0.7691\n",
            "Epoch 128/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5296 - acc: 0.7575 - val_loss: 0.5031 - val_acc: 0.7688\n",
            "Epoch 129/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5296 - acc: 0.7574 - val_loss: 0.5021 - val_acc: 0.7695\n",
            "Epoch 130/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5307 - acc: 0.7568 - val_loss: 0.5020 - val_acc: 0.7692\n",
            "Epoch 131/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5292 - acc: 0.7578 - val_loss: 0.5017 - val_acc: 0.7698\n",
            "Epoch 132/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5291 - acc: 0.7577 - val_loss: 0.5014 - val_acc: 0.7698\n",
            "Epoch 133/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5292 - acc: 0.7576 - val_loss: 0.5015 - val_acc: 0.7698\n",
            "Epoch 134/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5291 - acc: 0.7581 - val_loss: 0.5018 - val_acc: 0.7700\n",
            "Epoch 135/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5289 - acc: 0.7582 - val_loss: 0.5013 - val_acc: 0.7698\n",
            "Epoch 136/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5288 - acc: 0.7584 - val_loss: 0.5017 - val_acc: 0.7700\n",
            "Epoch 137/500\n",
            "4061/4061 [==============================] - 82s 20ms/step - loss: 0.5287 - acc: 0.7585 - val_loss: 0.5015 - val_acc: 0.7700\n",
            "Epoch 138/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5286 - acc: 0.7585 - val_loss: 0.5010 - val_acc: 0.7703\n",
            "Epoch 139/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5288 - acc: 0.7582 - val_loss: 0.5016 - val_acc: 0.7700\n",
            "Epoch 140/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5284 - acc: 0.7587 - val_loss: 0.5010 - val_acc: 0.7704\n",
            "Epoch 141/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5284 - acc: 0.7586 - val_loss: 0.5013 - val_acc: 0.7707\n",
            "Epoch 142/500\n",
            "4061/4061 [==============================] - 82s 20ms/step - loss: 0.5286 - acc: 0.7585 - val_loss: 0.5026 - val_acc: 0.7696\n",
            "Epoch 143/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5282 - acc: 0.7588 - val_loss: 0.5004 - val_acc: 0.7707\n",
            "Epoch 144/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5279 - acc: 0.7588 - val_loss: 0.5005 - val_acc: 0.7704\n",
            "Epoch 145/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5280 - acc: 0.7587 - val_loss: 0.5004 - val_acc: 0.7706\n",
            "Epoch 146/500\n",
            "4061/4061 [==============================] - 82s 20ms/step - loss: 0.5279 - acc: 0.7590 - val_loss: 0.5015 - val_acc: 0.7701\n",
            "Epoch 147/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5277 - acc: 0.7590 - val_loss: 0.5017 - val_acc: 0.7702\n",
            "Epoch 148/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5275 - acc: 0.7592 - val_loss: 0.5005 - val_acc: 0.7708\n",
            "Epoch 149/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5274 - acc: 0.7592 - val_loss: 0.5014 - val_acc: 0.7705\n",
            "Epoch 150/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5273 - acc: 0.7594 - val_loss: 0.5003 - val_acc: 0.7709\n",
            "Epoch 151/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5274 - acc: 0.7594 - val_loss: 0.4999 - val_acc: 0.7712\n",
            "Epoch 152/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5272 - acc: 0.7595 - val_loss: 0.4998 - val_acc: 0.7712\n",
            "Epoch 153/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5272 - acc: 0.7595 - val_loss: 0.4997 - val_acc: 0.7714\n",
            "Epoch 154/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5271 - acc: 0.7595 - val_loss: 0.4998 - val_acc: 0.7708\n",
            "Epoch 155/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5277 - acc: 0.7591 - val_loss: 0.5000 - val_acc: 0.7712\n",
            "Epoch 156/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5273 - acc: 0.7593 - val_loss: 0.4996 - val_acc: 0.7713\n",
            "Epoch 157/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5268 - acc: 0.7599 - val_loss: 0.4998 - val_acc: 0.7708\n",
            "Epoch 158/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5269 - acc: 0.7596 - val_loss: 0.5003 - val_acc: 0.7709\n",
            "Epoch 159/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5268 - acc: 0.7599 - val_loss: 0.4994 - val_acc: 0.7718\n",
            "Epoch 160/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5267 - acc: 0.7599 - val_loss: 0.4996 - val_acc: 0.7714\n",
            "Epoch 161/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5265 - acc: 0.7602 - val_loss: 0.4991 - val_acc: 0.7716\n",
            "Epoch 162/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5267 - acc: 0.7599 - val_loss: 0.5020 - val_acc: 0.7694\n",
            "Epoch 163/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5266 - acc: 0.7600 - val_loss: 0.4989 - val_acc: 0.7718\n",
            "Epoch 164/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5263 - acc: 0.7604 - val_loss: 0.4993 - val_acc: 0.7717\n",
            "Epoch 165/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5262 - acc: 0.7603 - val_loss: 0.4992 - val_acc: 0.7716\n",
            "Epoch 166/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5264 - acc: 0.7599 - val_loss: 0.4992 - val_acc: 0.7718\n",
            "Epoch 167/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5262 - acc: 0.7602 - val_loss: 0.4990 - val_acc: 0.7718\n",
            "Epoch 168/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5319 - acc: 0.7564 - val_loss: 0.5015 - val_acc: 0.7699\n",
            "Epoch 169/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5278 - acc: 0.7592 - val_loss: 0.4994 - val_acc: 0.7715\n",
            "Epoch 170/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5267 - acc: 0.7601 - val_loss: 0.5001 - val_acc: 0.7714\n",
            "Epoch 171/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5264 - acc: 0.7603 - val_loss: 0.4990 - val_acc: 0.7717\n",
            "Epoch 172/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5261 - acc: 0.7604 - val_loss: 0.4986 - val_acc: 0.7717\n",
            "Epoch 173/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5261 - acc: 0.7604 - val_loss: 0.5007 - val_acc: 0.7712\n",
            "Epoch 174/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5260 - acc: 0.7605 - val_loss: 0.4986 - val_acc: 0.7721\n",
            "Epoch 175/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5260 - acc: 0.7605 - val_loss: 0.4994 - val_acc: 0.7719\n",
            "Epoch 176/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5258 - acc: 0.7605 - val_loss: 0.4989 - val_acc: 0.7723\n",
            "Epoch 177/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5263 - acc: 0.7603 - val_loss: 0.4996 - val_acc: 0.7719\n",
            "Epoch 178/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5260 - acc: 0.7606 - val_loss: 0.4993 - val_acc: 0.7721\n",
            "Epoch 179/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5258 - acc: 0.7608 - val_loss: 0.4983 - val_acc: 0.7722\n",
            "Epoch 180/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5257 - acc: 0.7608 - val_loss: 0.4982 - val_acc: 0.7724\n",
            "Epoch 181/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5256 - acc: 0.7608 - val_loss: 0.4982 - val_acc: 0.7722\n",
            "Epoch 182/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5256 - acc: 0.7609 - val_loss: 0.4987 - val_acc: 0.7723\n",
            "Epoch 183/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5259 - acc: 0.7607 - val_loss: 0.4984 - val_acc: 0.7724\n",
            "Epoch 184/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5255 - acc: 0.7610 - val_loss: 0.4983 - val_acc: 0.7719\n",
            "Epoch 185/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5256 - acc: 0.7609 - val_loss: 0.4981 - val_acc: 0.7724\n",
            "Epoch 186/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5254 - acc: 0.7610 - val_loss: 0.4986 - val_acc: 0.7724\n",
            "Epoch 187/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5253 - acc: 0.7611 - val_loss: 0.4989 - val_acc: 0.7724\n",
            "Epoch 188/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5254 - acc: 0.7612 - val_loss: 0.4979 - val_acc: 0.7725\n",
            "Epoch 189/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5255 - acc: 0.7610 - val_loss: 0.4982 - val_acc: 0.7725\n",
            "Epoch 190/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5254 - acc: 0.7611 - val_loss: 0.4985 - val_acc: 0.7724\n",
            "Epoch 191/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5254 - acc: 0.7611 - val_loss: 0.4982 - val_acc: 0.7726\n",
            "Epoch 192/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5254 - acc: 0.7611 - val_loss: 0.4982 - val_acc: 0.7726\n",
            "Epoch 193/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5253 - acc: 0.7612 - val_loss: 0.4979 - val_acc: 0.7724\n",
            "Epoch 194/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5251 - acc: 0.7613 - val_loss: 0.4978 - val_acc: 0.7726\n",
            "Epoch 195/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5252 - acc: 0.7612 - val_loss: 0.4986 - val_acc: 0.7723\n",
            "Epoch 196/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5254 - acc: 0.7611 - val_loss: 0.4986 - val_acc: 0.7722\n",
            "Epoch 197/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5250 - acc: 0.7612 - val_loss: 0.4989 - val_acc: 0.7714\n",
            "Epoch 198/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5251 - acc: 0.7612 - val_loss: 0.4981 - val_acc: 0.7720\n",
            "Epoch 199/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5250 - acc: 0.7614 - val_loss: 0.4982 - val_acc: 0.7722\n",
            "Epoch 200/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5251 - acc: 0.7613 - val_loss: 0.4989 - val_acc: 0.7724\n",
            "Epoch 201/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5248 - acc: 0.7614 - val_loss: 0.4976 - val_acc: 0.7728\n",
            "Epoch 202/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5253 - acc: 0.7612 - val_loss: 0.4976 - val_acc: 0.7729\n",
            "Epoch 203/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5246 - acc: 0.7617 - val_loss: 0.4997 - val_acc: 0.7719\n",
            "Epoch 204/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5249 - acc: 0.7614 - val_loss: 0.4979 - val_acc: 0.7726\n",
            "Epoch 205/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5265 - acc: 0.7606 - val_loss: 0.4978 - val_acc: 0.7727\n",
            "Epoch 206/500\n",
            "4061/4061 [==============================] - 77s 19ms/step - loss: 0.5248 - acc: 0.7615 - val_loss: 0.4973 - val_acc: 0.7724\n",
            "Epoch 207/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5246 - acc: 0.7616 - val_loss: 0.4982 - val_acc: 0.7720\n",
            "Epoch 208/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5247 - acc: 0.7615 - val_loss: 0.4988 - val_acc: 0.7725\n",
            "Epoch 209/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5247 - acc: 0.7615 - val_loss: 0.4975 - val_acc: 0.7724\n",
            "Epoch 210/500\n",
            "4061/4061 [==============================] - 86s 21ms/step - loss: 0.5245 - acc: 0.7617 - val_loss: 0.4982 - val_acc: 0.7726\n",
            "Epoch 211/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5315 - acc: 0.7582 - val_loss: 0.4980 - val_acc: 0.7721\n",
            "Epoch 212/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5248 - acc: 0.7615 - val_loss: 0.4972 - val_acc: 0.7728\n",
            "Epoch 213/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5244 - acc: 0.7618 - val_loss: 0.4975 - val_acc: 0.7727\n",
            "Epoch 214/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5244 - acc: 0.7617 - val_loss: 0.4976 - val_acc: 0.7729\n",
            "Epoch 215/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5250 - acc: 0.7614 - val_loss: 0.4969 - val_acc: 0.7730\n",
            "Epoch 216/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5242 - acc: 0.7617 - val_loss: 0.4973 - val_acc: 0.7728\n",
            "Epoch 217/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5243 - acc: 0.7618 - val_loss: 0.4973 - val_acc: 0.7730\n",
            "Epoch 218/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5243 - acc: 0.7618 - val_loss: 0.4970 - val_acc: 0.7731\n",
            "Epoch 219/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5242 - acc: 0.7618 - val_loss: 0.4970 - val_acc: 0.7730\n",
            "Epoch 220/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5241 - acc: 0.7618 - val_loss: 0.4970 - val_acc: 0.7731\n",
            "Epoch 221/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5243 - acc: 0.7618 - val_loss: 0.5003 - val_acc: 0.7714\n",
            "Epoch 222/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5241 - acc: 0.7619 - val_loss: 0.4973 - val_acc: 0.7733\n",
            "Epoch 223/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5241 - acc: 0.7619 - val_loss: 0.4969 - val_acc: 0.7731\n",
            "Epoch 224/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5245 - acc: 0.7617 - val_loss: 0.4983 - val_acc: 0.7726\n",
            "Epoch 225/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5243 - acc: 0.7620 - val_loss: 0.4966 - val_acc: 0.7732\n",
            "Epoch 226/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5256 - acc: 0.7612 - val_loss: 0.5022 - val_acc: 0.7717\n",
            "Epoch 227/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5249 - acc: 0.7615 - val_loss: 0.4966 - val_acc: 0.7733\n",
            "Epoch 228/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5241 - acc: 0.7619 - val_loss: 0.4973 - val_acc: 0.7732\n",
            "Epoch 229/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5240 - acc: 0.7621 - val_loss: 0.4968 - val_acc: 0.7732\n",
            "Epoch 230/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5240 - acc: 0.7621 - val_loss: 0.4966 - val_acc: 0.7733\n",
            "Epoch 231/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5239 - acc: 0.7621 - val_loss: 0.4969 - val_acc: 0.7731\n",
            "Epoch 232/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5239 - acc: 0.7620 - val_loss: 0.4965 - val_acc: 0.7732\n",
            "Epoch 233/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5241 - acc: 0.7619 - val_loss: 0.4972 - val_acc: 0.7731\n",
            "Epoch 234/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5237 - acc: 0.7623 - val_loss: 0.4967 - val_acc: 0.7735\n",
            "Epoch 235/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5239 - acc: 0.7622 - val_loss: 0.4966 - val_acc: 0.7734\n",
            "Epoch 236/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5237 - acc: 0.7623 - val_loss: 0.4970 - val_acc: 0.7732\n",
            "Epoch 237/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5240 - acc: 0.7619 - val_loss: 0.4967 - val_acc: 0.7733\n",
            "Epoch 238/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5237 - acc: 0.7622 - val_loss: 0.4961 - val_acc: 0.7736\n",
            "Epoch 239/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5237 - acc: 0.7624 - val_loss: 0.4971 - val_acc: 0.7734\n",
            "Epoch 240/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5239 - acc: 0.7623 - val_loss: 0.4968 - val_acc: 0.7733\n",
            "Epoch 241/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5235 - acc: 0.7625 - val_loss: 0.4964 - val_acc: 0.7735\n",
            "Epoch 242/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5237 - acc: 0.7623 - val_loss: 0.4968 - val_acc: 0.7736\n",
            "Epoch 243/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5236 - acc: 0.7623 - val_loss: 0.4970 - val_acc: 0.7731\n",
            "Epoch 244/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5241 - acc: 0.7620 - val_loss: 0.4968 - val_acc: 0.7733\n",
            "Epoch 245/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5239 - acc: 0.7622 - val_loss: 0.4961 - val_acc: 0.7737\n",
            "Epoch 246/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5235 - acc: 0.7626 - val_loss: 0.4971 - val_acc: 0.7736\n",
            "Epoch 247/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5235 - acc: 0.7625 - val_loss: 0.4980 - val_acc: 0.7727\n",
            "Epoch 248/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5235 - acc: 0.7625 - val_loss: 0.4962 - val_acc: 0.7733\n",
            "Epoch 249/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5234 - acc: 0.7624 - val_loss: 0.4961 - val_acc: 0.7739\n",
            "Epoch 250/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5235 - acc: 0.7624 - val_loss: 0.4963 - val_acc: 0.7734\n",
            "Epoch 251/500\n",
            "4061/4061 [==============================] - 77s 19ms/step - loss: 0.5236 - acc: 0.7624 - val_loss: 0.4960 - val_acc: 0.7737\n",
            "Epoch 252/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5251 - acc: 0.7615 - val_loss: 0.4994 - val_acc: 0.7729\n",
            "Epoch 253/500\n",
            "4061/4061 [==============================] - 77s 19ms/step - loss: 0.5241 - acc: 0.7621 - val_loss: 0.4963 - val_acc: 0.7734\n",
            "Epoch 254/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5231 - acc: 0.7626 - val_loss: 0.4957 - val_acc: 0.7737\n",
            "Epoch 255/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5232 - acc: 0.7627 - val_loss: 0.4957 - val_acc: 0.7738\n",
            "Epoch 256/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5232 - acc: 0.7627 - val_loss: 0.4958 - val_acc: 0.7739\n",
            "Epoch 257/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5964 - acc: 0.7412 - val_loss: 0.5569 - val_acc: 0.7484\n",
            "Epoch 258/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5717 - acc: 0.7388 - val_loss: 0.5283 - val_acc: 0.7567\n",
            "Epoch 259/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5520 - acc: 0.7451 - val_loss: 0.5143 - val_acc: 0.7628\n",
            "Epoch 260/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5397 - acc: 0.7516 - val_loss: 0.5049 - val_acc: 0.7671\n",
            "Epoch 261/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5328 - acc: 0.7563 - val_loss: 0.5012 - val_acc: 0.7705\n",
            "Epoch 262/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5298 - acc: 0.7587 - val_loss: 0.4995 - val_acc: 0.7718\n",
            "Epoch 263/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5280 - acc: 0.7598 - val_loss: 0.4983 - val_acc: 0.7722\n",
            "Epoch 264/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5269 - acc: 0.7603 - val_loss: 0.4976 - val_acc: 0.7727\n",
            "Epoch 265/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5260 - acc: 0.7606 - val_loss: 0.4972 - val_acc: 0.7726\n",
            "Epoch 266/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5253 - acc: 0.7608 - val_loss: 0.4970 - val_acc: 0.7728\n",
            "Epoch 267/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5247 - acc: 0.7611 - val_loss: 0.4970 - val_acc: 0.7729\n",
            "Epoch 268/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5242 - acc: 0.7615 - val_loss: 0.4964 - val_acc: 0.7732\n",
            "Epoch 269/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5237 - acc: 0.7618 - val_loss: 0.4962 - val_acc: 0.7734\n",
            "Epoch 270/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5234 - acc: 0.7621 - val_loss: 0.4961 - val_acc: 0.7735\n",
            "Epoch 271/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5232 - acc: 0.7622 - val_loss: 0.4960 - val_acc: 0.7735\n",
            "Epoch 272/500\n",
            "4061/4061 [==============================] - 83s 20ms/step - loss: 0.5231 - acc: 0.7623 - val_loss: 0.4960 - val_acc: 0.7736\n",
            "Epoch 273/500\n",
            "4061/4061 [==============================] - 83s 21ms/step - loss: 0.5230 - acc: 0.7623 - val_loss: 0.4963 - val_acc: 0.7734\n",
            "Epoch 274/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5230 - acc: 0.7623 - val_loss: 0.4958 - val_acc: 0.7738\n",
            "Epoch 275/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5229 - acc: 0.7624 - val_loss: 0.4961 - val_acc: 0.7735\n",
            "Epoch 276/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5229 - acc: 0.7624 - val_loss: 0.4957 - val_acc: 0.7738\n",
            "Epoch 277/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5229 - acc: 0.7624 - val_loss: 0.4959 - val_acc: 0.7737\n",
            "Epoch 278/500\n",
            "4061/4061 [==============================] - 78s 19ms/step - loss: 0.5229 - acc: 0.7624 - val_loss: 0.4962 - val_acc: 0.7735\n",
            "Epoch 279/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5229 - acc: 0.7628 - val_loss: 0.4959 - val_acc: 0.7738\n",
            "Epoch 280/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5228 - acc: 0.7628 - val_loss: 0.4956 - val_acc: 0.7738\n",
            "Epoch 281/500\n",
            "4061/4061 [==============================] - 79s 19ms/step - loss: 0.5228 - acc: 0.7628 - val_loss: 0.4956 - val_acc: 0.7739\n",
            "Epoch 282/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5227 - acc: 0.7629 - val_loss: 0.4962 - val_acc: 0.7734\n",
            "Epoch 283/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5228 - acc: 0.7630 - val_loss: 0.4961 - val_acc: 0.7736\n",
            "Epoch 284/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5228 - acc: 0.7630 - val_loss: 0.4958 - val_acc: 0.7737\n",
            "Epoch 285/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5227 - acc: 0.7630 - val_loss: 0.4956 - val_acc: 0.7739\n",
            "Epoch 286/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5227 - acc: 0.7629 - val_loss: 0.4965 - val_acc: 0.7736\n",
            "Epoch 287/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5229 - acc: 0.7629 - val_loss: 0.4957 - val_acc: 0.7739\n",
            "Epoch 288/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5227 - acc: 0.7629 - val_loss: 0.4963 - val_acc: 0.7736\n",
            "Epoch 289/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5228 - acc: 0.7629 - val_loss: 0.4962 - val_acc: 0.7736\n",
            "Epoch 290/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5305 - acc: 0.7590 - val_loss: 0.4969 - val_acc: 0.7736\n",
            "Epoch 291/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5233 - acc: 0.7627 - val_loss: 0.4961 - val_acc: 0.7736\n",
            "Epoch 292/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5229 - acc: 0.7628 - val_loss: 0.4956 - val_acc: 0.7737\n",
            "Epoch 293/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5228 - acc: 0.7628 - val_loss: 0.4965 - val_acc: 0.7736\n",
            "Epoch 294/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5227 - acc: 0.7630 - val_loss: 0.4960 - val_acc: 0.7737\n",
            "Epoch 295/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5227 - acc: 0.7629 - val_loss: 0.4955 - val_acc: 0.7738\n",
            "Epoch 296/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5226 - acc: 0.7630 - val_loss: 0.4955 - val_acc: 0.7738\n",
            "Epoch 297/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5226 - acc: 0.7630 - val_loss: 0.4959 - val_acc: 0.7737\n",
            "Epoch 298/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5230 - acc: 0.7629 - val_loss: 0.4960 - val_acc: 0.7748\n",
            "Epoch 299/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5231 - acc: 0.7629 - val_loss: 0.4954 - val_acc: 0.7738\n",
            "Epoch 300/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5227 - acc: 0.7629 - val_loss: 0.4972 - val_acc: 0.7731\n",
            "Epoch 301/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5228 - acc: 0.7629 - val_loss: 0.4961 - val_acc: 0.7739\n",
            "Epoch 302/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5229 - acc: 0.7628 - val_loss: 0.4954 - val_acc: 0.7741\n",
            "Epoch 303/500\n",
            "4061/4061 [==============================] - 83s 20ms/step - loss: 0.5229 - acc: 0.7627 - val_loss: 0.4955 - val_acc: 0.7740\n",
            "Epoch 304/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5227 - acc: 0.7630 - val_loss: 0.4954 - val_acc: 0.7741\n",
            "Epoch 305/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5227 - acc: 0.7629 - val_loss: 0.4958 - val_acc: 0.7739\n",
            "Epoch 306/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5226 - acc: 0.7629 - val_loss: 0.4963 - val_acc: 0.7737\n",
            "Epoch 307/500\n",
            "4061/4061 [==============================] - 82s 20ms/step - loss: 0.5226 - acc: 0.7631 - val_loss: 0.4953 - val_acc: 0.7741\n",
            "Epoch 308/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5227 - acc: 0.7631 - val_loss: 0.4955 - val_acc: 0.7739\n",
            "Epoch 309/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5226 - acc: 0.7631 - val_loss: 0.4955 - val_acc: 0.7741\n",
            "Epoch 310/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5227 - acc: 0.7631 - val_loss: 0.4954 - val_acc: 0.7741\n",
            "Epoch 311/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5228 - acc: 0.7632 - val_loss: 0.4955 - val_acc: 0.7743\n",
            "Epoch 312/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5225 - acc: 0.7632 - val_loss: 0.4952 - val_acc: 0.7743\n",
            "Epoch 313/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5227 - acc: 0.7631 - val_loss: 0.4958 - val_acc: 0.7740\n",
            "Epoch 314/500\n",
            "4061/4061 [==============================] - 81s 20ms/step - loss: 0.5227 - acc: 0.7630 - val_loss: 0.4958 - val_acc: 0.7740\n",
            "Epoch 315/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5226 - acc: 0.7631 - val_loss: 0.4957 - val_acc: 0.7741\n",
            "Epoch 316/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5227 - acc: 0.7631 - val_loss: 0.4954 - val_acc: 0.7743\n",
            "Epoch 317/500\n",
            "4061/4061 [==============================] - 79s 20ms/step - loss: 0.5225 - acc: 0.7633 - val_loss: 0.4966 - val_acc: 0.7737\n",
            "Epoch 318/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5227 - acc: 0.7631 - val_loss: 0.4965 - val_acc: 0.7736\n",
            "Epoch 319/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5224 - acc: 0.7634 - val_loss: 0.4954 - val_acc: 0.7741\n",
            "Epoch 320/500\n",
            "4061/4061 [==============================] - 80s 20ms/step - loss: 0.5225 - acc: 0.7632 - val_loss: 0.4984 - val_acc: 0.7726\n",
            "Epoch 321/500\n",
            "4032/4061 [============================>.] - ETA: 0s - loss: 0.5224 - acc: 0.7635"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-c84f1e47454c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0mfinish_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    210\u001b[0m                         val_outs = test_loop(model, val_f, val_ins,\n\u001b[1;32m    211\u001b[0m                                              \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m                                              verbose=0)\n\u001b[0m\u001b[1;32m    213\u001b[0m                         \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                         \u001b[0;31m# Same labels assumed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pE5tuzfRS9SK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXOucdnVW4mF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "1565395a-3cdb-412d-dc97-7c9ea5293500"
      },
      "source": [
        "import random as rnd\n",
        "row=rnd.randint(1,500)\n",
        "clm=rnd.randint(1,100)\n",
        "\n",
        "print('row: ',row)\n",
        "print('column: ',clm)\n",
        "\n",
        "big=max(y_pred[row][clm])\n",
        "arr=[]\n",
        "for item in y_pred[row][clm]:\n",
        "  if(item==big):\n",
        "    arr.append(1)\n",
        "  else:\n",
        "    arr.append(0)\n",
        "    \n",
        "print('actual: ',y_test[row][clm])    \n",
        "print('predicted: ',arr)\n",
        "\n",
        "plt.plot(y_test[row][clm])\n",
        "plt.plot(arr)\n",
        "plt.show()"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "row:  183\n",
            "column:  27\n",
            "actual:  [0 0 1 0 0]\n",
            "predicted:  [0, 0, 1, 0, 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nI5Sr1AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}